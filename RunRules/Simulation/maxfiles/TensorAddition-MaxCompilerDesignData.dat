#ifdef MAXCOMPILER_VERSION_INFO
#define MAXCOMPILER_VERSION_INFO_PRESENT 1
#define MAXFILE_MAXCOMPILER_VERSION_YEAR         2015
#define MAXFILE_MAXCOMPILER_VERSION_NUM          2
#define MAXFILE_MAXCOMPILER_VERSION_POINT        0
#define MAXFILE_MAXCOMPILER_VERSION_PATCH        ""
#define MAXFILE_MAXCOMPILER_VERSION_REV          46290
#define MAXFILE_MAXCOMPILER_VERSION_RELEASE_DATE "2015-11-17"
#define MAXFILE_MAXCOMPILER_VERSION_RELEASE_MODE true
#endif

#ifdef MAXFILE_BUILD_INFO
#define MAXFILE_BUILD_INFO_PRESENT 1
#define MAXFILE_BUILD_NAME "TensorAddition"
#define MAXFILE_BUILD_DIR  "/home/dpesic/workspace/TensorAddition/RunRules/Simulation/maxfiles/TensorAddition_VECTIS_DFE_SIM"
#define MAXFILE_BUILD_DATE  20160222
#define MAXFILE_BUILD_REV   1
#endif

#ifdef PARAM
#define PARAM_PRESENT 1
PARAM(APP_ID, 0)
PARAM(REV_ID, 0)
PARAM(CHAIN_LENGTH, 36)
PARAM(IS_SIMULATION, 1)
PARAM(MEC_SUPPORTED, 1)
PARAM(PCIE_SLAVE_STREAMING, 0)
PARAM(PCIE_ALIGNMENT, 16)
PARAM(NUM_IFPGA_LINKS, 0)
#endif

#ifdef STRING_PARAM
#define STRING_PARAM_PRESENT 1
STRING_PARAM(BOARD_MODEL, "MAX3424A")
#endif



#ifdef INCLUDE_GENERATED_CPP_HEADERS
#include "TensorAdditionKernel.h"
#endif










#ifdef MANAGER_NODE
#define MANAGER_NODE_PRESENT 1
MANAGER_NODE(TensorAdditionKernel, Kernel)
MANAGER_NODE(x, PCIe_From_Host_fwd)
MANAGER_NODE(y, PCIe_From_Host_fwd)
MANAGER_NODE(s, PCIe_To_Host_fwd)
MANAGER_NODE(Stream_1, DualAspectMux)
MANAGER_NODE(Stream_4, DualAspectMux)
MANAGER_NODE(Stream_9, DualAspectReg)
MANAGER_NODE(Stream_21, StreamPullPushAdapter)
MANAGER_NODE(Stream_13, Fifo)
MANAGER_NODE(Stream_17, Fifo)
MANAGER_NODE(Stream_19, Fifo)
#endif

#ifdef MANAGER_NODE_IO
#define MANAGER_NODE_IO_PRESENT 1
MANAGER_NODE_IO(TensorAdditionKernel, x, IN, STREAM, 64, PULL)
MANAGER_NODE_IO(TensorAdditionKernel, y, IN, STREAM, 64, PULL)
MANAGER_NODE_IO(TensorAdditionKernel, s, OUT, STREAM, 64, PUSH)
MANAGER_NODE_IO(x, x, OUT, STREAM, 128, PULL)
MANAGER_NODE_IO(y, y, OUT, STREAM, 128, PULL)
MANAGER_NODE_IO(s, s, IN, STREAM, 128, PUSH)
MANAGER_NODE_IO(Stream_1, input, IN, STREAM, 128, PULL)
MANAGER_NODE_IO(Stream_1, output, OUT, STREAM, 64, PUSH)
MANAGER_NODE_IO(Stream_4, input, IN, STREAM, 128, PULL)
MANAGER_NODE_IO(Stream_4, output, OUT, STREAM, 64, PUSH)
MANAGER_NODE_IO(Stream_9, input, IN, STREAM, 64, PULL)
MANAGER_NODE_IO(Stream_9, output, OUT, STREAM, 128, PULL)
MANAGER_NODE_IO(Stream_21, input, IN, STREAM, 128, PULL)
MANAGER_NODE_IO(Stream_21, output, OUT, STREAM, 128, PUSH)
MANAGER_NODE_IO(Stream_13, input, IN, STREAM, 64, PUSH)
MANAGER_NODE_IO(Stream_13, output, OUT, STREAM, 64, PULL)
MANAGER_NODE_IO(Stream_17, input, IN, STREAM, 64, PUSH)
MANAGER_NODE_IO(Stream_17, output, OUT, STREAM, 64, PULL)
MANAGER_NODE_IO(Stream_19, input, IN, STREAM, 64, PUSH)
MANAGER_NODE_IO(Stream_19, output, OUT, STREAM, 64, PULL)
#endif

#ifdef MANAGER_STREAM
#define MANAGER_STREAM_PRESENT 1
MANAGER_STREAM(x, x, Stream_1, input, 128)
MANAGER_STREAM(y, y, Stream_4, input, 128)
MANAGER_STREAM(TensorAdditionKernel, s, Stream_19, input, 64)
MANAGER_STREAM(Stream_1, output, Stream_13, input, 64)
MANAGER_STREAM(Stream_4, output, Stream_17, input, 64)
MANAGER_STREAM(Stream_9, output, Stream_21, input, 128)
MANAGER_STREAM(Stream_21, output, s, s, 128)
MANAGER_STREAM(Stream_13, output, TensorAdditionKernel, x, 64)
MANAGER_STREAM(Stream_17, output, TensorAdditionKernel, y, 64)
MANAGER_STREAM(Stream_19, output, Stream_9, input, 64)
#endif

#ifdef MANAGER_NODE_STACK_TRACE
#define MANAGER_NODE_STACK_TRACE_PRESENT 1
MANAGER_NODE_STACK_TRACE(TensorAdditionKernel, "com.maxeler.maxcompiler.v2.managers.DFEManager.build(DFEManager.java:373)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:32)\n")
MANAGER_NODE_STACK_TRACE(x, "com.maxeler.maxcompiler.v2.managers.DFEManager.build(DFEManager.java:373)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:32)\n")
MANAGER_NODE_STACK_TRACE(y, "com.maxeler.maxcompiler.v2.managers.DFEManager.build(DFEManager.java:373)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:32)\n")
MANAGER_NODE_STACK_TRACE(s, "com.maxeler.maxcompiler.v2.managers.DFEManager.build(DFEManager.java:373)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:32)\n")
MANAGER_NODE_STACK_TRACE(Stream_1, "com.maxeler.maxcompiler.v2.managers.DFEManager.build(DFEManager.java:373)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:32)\n")
MANAGER_NODE_STACK_TRACE(Stream_4, "com.maxeler.maxcompiler.v2.managers.DFEManager.build(DFEManager.java:373)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:32)\n")
MANAGER_NODE_STACK_TRACE(Stream_9, "com.maxeler.maxcompiler.v2.managers.DFEManager.build(DFEManager.java:373)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:32)\n")
MANAGER_NODE_STACK_TRACE(Stream_21, "com.maxeler.maxcompiler.v2.managers.DFEManager.build(DFEManager.java:373)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:32)\n")
MANAGER_NODE_STACK_TRACE(Stream_13, "com.maxeler.maxcompiler.v2.managers.DFEManager.build(DFEManager.java:373)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:32)\n")
MANAGER_NODE_STACK_TRACE(Stream_17, "com.maxeler.maxcompiler.v2.managers.DFEManager.build(DFEManager.java:373)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:32)\n")
MANAGER_NODE_STACK_TRACE(Stream_19, "com.maxeler.maxcompiler.v2.managers.DFEManager.build(DFEManager.java:373)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:32)\n")
#endif


#ifdef MANAGER_NODE_PROPERTY
#define MANAGER_NODE_PROPERTY_PRESENT 1
MANAGER_NODE_PROPERTY(TensorAdditionKernel, control_pipelining_depth, 2)
#endif


#ifdef KERNEL_CORE
#define KERNEL_CORE_PRESENT 1
KERNEL_CORE(TensorAdditionKernel)
#endif

#ifdef KERNEL_HOST_CONTROLLED
#define KERNEL_HOST_CONTROLLED_PRESENT 1
KERNEL_HOST_CONTROLLED(TensorAdditionKernel, TensorAdditionKernel)
#endif

#ifdef DEBUG_INPUT_BITS
#define DEBUG_INPUT_BITS_PRESENT 1
DEBUG_INPUT_BITS(TensorAdditionKernel, x, 0)
DEBUG_INPUT_BITS(TensorAdditionKernel, y, 1)
#endif

#ifdef DEBUG_OUTPUT_BITS
#define DEBUG_OUTPUT_BITS_PRESENT 1
DEBUG_OUTPUT_BITS(TensorAdditionKernel, s, 0)
#endif

#ifdef MANAGER_NODE_CPP_SIM_MODEL_CTOR
#define MANAGER_NODE_CPP_SIM_MODEL_CTOR_PRESENT 1
MANAGER_NODE_CPP_SIM_MODEL_CTOR(x, PCIePullSourceSyncMax3, "x")
MANAGER_NODE_CPP_SIM_MODEL_CTOR(y, PCIePullSourceSyncMax3, "y")
MANAGER_NODE_CPP_SIM_MODEL_CTOR(Stream_1, DualAspectMuxSync, "Stream_1", 64, 2)
MANAGER_NODE_CPP_SIM_MODEL_CTOR(Stream_4, DualAspectMuxSync, "Stream_4", 64, 2)
MANAGER_NODE_CPP_SIM_MODEL_CTOR(Stream_13, FifoPushToPullSync, "Stream_13", false, 512, 64)
MANAGER_NODE_CPP_SIM_MODEL_CTOR(Stream_17, FifoPushToPullSync, "Stream_17", false, 512, 64)
MANAGER_NODE_CPP_SIM_MODEL_CTOR(TensorAdditionKernel, TensorAdditionKernel, "TensorAdditionKernel")
MANAGER_NODE_CPP_SIM_MODEL_CTOR(Stream_19, FifoPushToPullSync, "Stream_19", false, 512, 64)
MANAGER_NODE_CPP_SIM_MODEL_CTOR(Stream_9, DualAspectRegSync, "Stream_9", 64, 2)
MANAGER_NODE_CPP_SIM_MODEL_CTOR(Stream_21, PullToPushAdapterSync, "Stream_21")
MANAGER_NODE_CPP_SIM_MODEL_CTOR(s, PCIePushSinkSyncMax3, "s")
MANAGER_NODE_CPP_SIM_MODEL_CTOR(CapabilityReg, CapRegs, "CapabilityReg", 0, 2, 1, 0, 0, 0, 0, 0, 36)
MANAGER_NODE_CPP_SIM_MODEL_CTOR(ifpga, IFPGARegs)
MANAGER_NODE_CPP_SIM_MODEL_CTOR(sfa, SFARegs)
MANAGER_NODE_CPP_SIM_MODEL_CTOR(ChecksumMemory, ChecksumMem, "ChecksumMemory", "700d2e67b18154d7401959a33e27c826bc56d14a4b0e09bfaa4c783afd7fb8ff")
#endif

#ifdef MANAGER_NODE_CPP_SIM_MODEL_SETUP
#define MANAGER_NODE_CPP_SIM_MODEL_SETUP_PRESENT 1
MANAGER_NODE_CPP_SIM_MODEL_SETUP(TensorAdditionKernel, addDebugRegister, reg_dbg_ctld_almost_empty, 2, 0)
MANAGER_NODE_CPP_SIM_MODEL_SETUP(TensorAdditionKernel, addDebugRegister, reg_dbg_ctld_done, 2, 0)
MANAGER_NODE_CPP_SIM_MODEL_SETUP(TensorAdditionKernel, addDebugRegister, reg_dbg_ctld_empty, 2, 0)
MANAGER_NODE_CPP_SIM_MODEL_SETUP(TensorAdditionKernel, addDebugRegister, reg_dbg_ctld_read, 2, 0)
MANAGER_NODE_CPP_SIM_MODEL_SETUP(TensorAdditionKernel, addDebugRegister, reg_dbg_ctld_read_pipe_dbg, 6, 0)
MANAGER_NODE_CPP_SIM_MODEL_SETUP(TensorAdditionKernel, addDebugRegister, reg_dbg_ctld_request, 2, 0)
MANAGER_NODE_CPP_SIM_MODEL_SETUP(TensorAdditionKernel, addDebugRegister, reg_dbg_done_out, 1, 0)
MANAGER_NODE_CPP_SIM_MODEL_SETUP(TensorAdditionKernel, addDebugRegister, reg_dbg_fill_level, 5, 0)
MANAGER_NODE_CPP_SIM_MODEL_SETUP(TensorAdditionKernel, addDebugRegister, reg_dbg_flush_level, 5, 0)
MANAGER_NODE_CPP_SIM_MODEL_SETUP(TensorAdditionKernel, addDebugRegister, reg_dbg_flush_start, 1, 0)
MANAGER_NODE_CPP_SIM_MODEL_SETUP(TensorAdditionKernel, addDebugRegister, reg_dbg_flush_start_level, 5, 0)
MANAGER_NODE_CPP_SIM_MODEL_SETUP(TensorAdditionKernel, addDebugRegister, reg_dbg_flushing, 1, 0)
MANAGER_NODE_CPP_SIM_MODEL_SETUP(TensorAdditionKernel, addDebugRegister, reg_dbg_full_level, 5, 0)
MANAGER_NODE_CPP_SIM_MODEL_SETUP(TensorAdditionKernel, addDebugRegister, reg_dbg_out_stall, 1, 0)
MANAGER_NODE_CPP_SIM_MODEL_SETUP(TensorAdditionKernel, addDebugRegister, reg_dbg_out_valid, 1, 0)
MANAGER_NODE_CPP_SIM_MODEL_SETUP(TensorAdditionKernel, addDebugRegister, reg_dbg_stall_vector, 1, 0)
#endif

#ifdef PCIE_STREAM
#define PCIE_STREAM_PRESENT 1
PCIE_STREAM(x, STREAM_FROM_HOST, 0)
PCIE_STREAM(y, STREAM_FROM_HOST, 1)
PCIE_STREAM(s, STREAM_TO_HOST, 0)
#endif

#ifdef REG
#define REG_PRESENT 1
REG(TensorAdditionKernel.io_x_force_disabled, 0x0, 1, hwOffsetFix(1, 0, UNSIGNED))
REG(TensorAdditionKernel.io_y_force_disabled, 0x1, 1, hwOffsetFix(1, 0, UNSIGNED))
REG(TensorAdditionKernel.io_s_force_disabled, 0x2, 1, hwOffsetFix(1, 0, UNSIGNED))
REG(TensorAdditionKernel.run_cycle_count, 0x3, 6, hwOffsetFix(48, 0, UNSIGNED))
REG(TensorAdditionKernel.current_run_cycle_count, 0x9, 6, hwOffsetFix(48, 0, UNSIGNED))
REG(TensorAdditionKernel.dbg_ctld_almost_empty, 0xf, 1, hwBits(2))
REG(TensorAdditionKernel.dbg_ctld_done, 0x10, 1, hwBits(2))
REG(TensorAdditionKernel.dbg_ctld_empty, 0x11, 1, hwBits(2))
REG(TensorAdditionKernel.dbg_ctld_read, 0x12, 1, hwBits(2))
REG(TensorAdditionKernel.dbg_ctld_read_pipe_dbg, 0x13, 1, hwBits(6))
REG(TensorAdditionKernel.dbg_ctld_request, 0x14, 1, hwBits(2))
REG(TensorAdditionKernel.dbg_done_out, 0x15, 1, hwBits(1))
REG(TensorAdditionKernel.dbg_fill_level, 0x16, 1, hwBits(5))
REG(TensorAdditionKernel.dbg_flush_level, 0x17, 1, hwBits(5))
REG(TensorAdditionKernel.dbg_flush_start, 0x18, 1, hwBits(1))
REG(TensorAdditionKernel.dbg_flush_start_level, 0x19, 1, hwBits(5))
REG(TensorAdditionKernel.dbg_flushing, 0x1a, 1, hwBits(1))
REG(TensorAdditionKernel.dbg_full_level, 0x1b, 1, hwBits(5))
REG(TensorAdditionKernel.dbg_out_stall, 0x1c, 1, hwBits(1))
REG(TensorAdditionKernel.dbg_out_valid, 0x1d, 1, hwBits(1))
REG(TensorAdditionKernel.dbg_stall_vector, 0x1e, 1, hwBits(1))
REG(ifpga.ifpga_ctrl, 0x1f, 1, hwBits(8))
REG(SignalForwardingAdapter.SFA_FORWARD_EN, 0x20, 4, hwBits(32))
#endif

#ifdef REG_V2
#define REG_V2_PRESENT 1
REG_V2(TensorAdditionKernel.io_x_force_disabled, 0x0, 1, hwOffsetFix(1, 0, UNSIGNED), HOST_WRITE_ONLY, true)
REG_V2(TensorAdditionKernel.io_y_force_disabled, 0x1, 1, hwOffsetFix(1, 0, UNSIGNED), HOST_WRITE_ONLY, true)
REG_V2(TensorAdditionKernel.io_s_force_disabled, 0x2, 1, hwOffsetFix(1, 0, UNSIGNED), HOST_WRITE_ONLY, true)
REG_V2(TensorAdditionKernel.run_cycle_count, 0x3, 6, hwOffsetFix(48, 0, UNSIGNED), HOST_WRITE_ONLY, false)
REG_V2(TensorAdditionKernel.current_run_cycle_count, 0x9, 6, hwOffsetFix(48, 0, UNSIGNED), HOST_READ_ONLY, false)
REG_V2(TensorAdditionKernel.dbg_ctld_almost_empty, 0xf, 1, hwBits(2), HOST_READ_ONLY, false)
REG_V2(TensorAdditionKernel.dbg_ctld_done, 0x10, 1, hwBits(2), HOST_READ_ONLY, false)
REG_V2(TensorAdditionKernel.dbg_ctld_empty, 0x11, 1, hwBits(2), HOST_READ_ONLY, false)
REG_V2(TensorAdditionKernel.dbg_ctld_read, 0x12, 1, hwBits(2), HOST_READ_ONLY, false)
REG_V2(TensorAdditionKernel.dbg_ctld_read_pipe_dbg, 0x13, 1, hwBits(6), HOST_READ_ONLY, false)
REG_V2(TensorAdditionKernel.dbg_ctld_request, 0x14, 1, hwBits(2), HOST_READ_ONLY, false)
REG_V2(TensorAdditionKernel.dbg_done_out, 0x15, 1, hwBits(1), HOST_READ_ONLY, false)
REG_V2(TensorAdditionKernel.dbg_fill_level, 0x16, 1, hwBits(5), HOST_READ_ONLY, false)
REG_V2(TensorAdditionKernel.dbg_flush_level, 0x17, 1, hwBits(5), HOST_READ_ONLY, false)
REG_V2(TensorAdditionKernel.dbg_flush_start, 0x18, 1, hwBits(1), HOST_READ_ONLY, false)
REG_V2(TensorAdditionKernel.dbg_flush_start_level, 0x19, 1, hwBits(5), HOST_READ_ONLY, false)
REG_V2(TensorAdditionKernel.dbg_flushing, 0x1a, 1, hwBits(1), HOST_READ_ONLY, false)
REG_V2(TensorAdditionKernel.dbg_full_level, 0x1b, 1, hwBits(5), HOST_READ_ONLY, false)
REG_V2(TensorAdditionKernel.dbg_out_stall, 0x1c, 1, hwBits(1), HOST_READ_ONLY, false)
REG_V2(TensorAdditionKernel.dbg_out_valid, 0x1d, 1, hwBits(1), HOST_READ_ONLY, false)
REG_V2(TensorAdditionKernel.dbg_stall_vector, 0x1e, 1, hwBits(1), HOST_READ_ONLY, false)
REG_V2(ifpga.ifpga_ctrl, 0x1f, 1, hwBits(8), HOST_READ_WRITE, false)
REG_V2(SignalForwardingAdapter.SFA_FORWARD_EN, 0x20, 4, hwBits(32), HOST_READ_WRITE, false)
#endif




#ifdef CHECKSUM
#define CHECKSUM_PRESENT 1
CHECKSUM("700d2e67b18154d7401959a33e27c826bc56d14a4b0e09bfaa4c783afd7fb8ff")
#endif




#ifdef CAPABILITY
#define CAPABILITY_PRESENT 1
CAPABILITY(MAX3REV, MAX3REVA)
CAPABILITY(MAX3RAM, DDR3_24GB)
CAPABILITY(MAX3FPGA, SXT475_2ES)
#endif

#ifdef DEFINE_DESIGN_NAME
#define DESIGN_NAME TensorAddition
#endif /* DEFINE_DESIGN_NAME */

#ifndef SLIC_NO_DECLARATIONS
/**\file */
#ifndef SLIC_DECLARATIONS_TensorAddition_H
#define SLIC_DECLARATIONS_TensorAddition_H
#include "MaxSLiCInterface.h"
#ifdef __cplusplus
extern "C" {
#endif /* __cplusplus */

#define TensorAddition_PCIE_ALIGNMENT (16)


/*----------------------------------------------------------------------------*/
/*---------------------------- Interface default -----------------------------*/
/*----------------------------------------------------------------------------*/




/**
 * \brief Basic static function for the interface 'default'.
 * 
 * \param [in] param_N Interface Parameter "N".
 * \param [in] instream_x The stream should be of size (param_N * 8) bytes.
 * \param [in] instream_y The stream should be of size (param_N * 8) bytes.
 * \param [out] outstream_s The stream should be of size (param_N * 8) bytes.
 */
void TensorAddition(
	int64_t param_N,
	const double *instream_x,
	const double *instream_y,
	double *outstream_s);

/**
 * \brief Basic static non-blocking function for the interface 'default'.
 * 
 * Schedule to run on an engine and return immediately.
 * The status of the run can be checked either by ::max_wait or ::max_nowait;
 * note that one of these *must* be called, so that associated memory can be released.
 * 
 * 
 * \param [in] param_N Interface Parameter "N".
 * \param [in] instream_x The stream should be of size (param_N * 8) bytes.
 * \param [in] instream_y The stream should be of size (param_N * 8) bytes.
 * \param [out] outstream_s The stream should be of size (param_N * 8) bytes.
 * \return A handle on the execution status, or NULL in case of error.
 */
max_run_t *TensorAddition_nonblock(
	int64_t param_N,
	const double *instream_x,
	const double *instream_y,
	double *outstream_s);

/**
 * \brief Advanced static interface, structure for the engine interface 'default'
 * 
 */
typedef struct { 
	int64_t param_N; /**<  [in] Interface Parameter "N". */
	const double *instream_x; /**<  [in] The stream should be of size (param_N * 8) bytes. */
	const double *instream_y; /**<  [in] The stream should be of size (param_N * 8) bytes. */
	double *outstream_s; /**<  [out] The stream should be of size (param_N * 8) bytes. */
} TensorAddition_actions_t;

/**
 * \brief Advanced static function for the interface 'default'.
 * 
 * \param [in] engine The engine on which the actions will be executed.
 * \param [in,out] interface_actions Actions to be executed.
 */
void TensorAddition_run(
	max_engine_t *engine,
	TensorAddition_actions_t *interface_actions);

/**
 * \brief Advanced static non-blocking function for the interface 'default'.
 *
 * Schedule the actions to run on the engine and return immediately.
 * The status of the run can be checked either by ::max_wait or ::max_nowait;
 * note that one of these *must* be called, so that associated memory can be released.
 *
 * 
 * \param [in] engine The engine on which the actions will be executed.
 * \param [in] interface_actions Actions to be executed.
 * \return A handle on the execution status of the actions, or NULL in case of error.
 */
max_run_t *TensorAddition_run_nonblock(
	max_engine_t *engine,
	TensorAddition_actions_t *interface_actions);

/**
 * \brief Group run advanced static function for the interface 'default'.
 * 
 * \param [in] group Group to use.
 * \param [in,out] interface_actions Actions to run.
 *
 * Run the actions on the first device available in the group.
 */
void TensorAddition_run_group(max_group_t *group, TensorAddition_actions_t *interface_actions);

/**
 * \brief Group run advanced static non-blocking function for the interface 'default'.
 * 
 *
 * Schedule the actions to run on the first device available in the group and return immediately.
 * The status of the run must be checked with ::max_wait. 
 * Note that use of ::max_nowait is prohibited with non-blocking running on groups:
 * see the ::max_run_group_nonblock documentation for more explanation.
 *
 * \param [in] group Group to use.
 * \param [in] interface_actions Actions to run.
 * \return A handle on the execution status of the actions, or NULL in case of error.
 */
max_run_t *TensorAddition_run_group_nonblock(max_group_t *group, TensorAddition_actions_t *interface_actions);

/**
 * \brief Array run advanced static function for the interface 'default'.
 * 
 * \param [in] engarray The array of devices to use.
 * \param [in,out] interface_actions The array of actions to run.
 *
 * Run the array of actions on the array of engines.  The length of interface_actions
 * must match the size of engarray.
 */
void TensorAddition_run_array(max_engarray_t *engarray, TensorAddition_actions_t *interface_actions[]);

/**
 * \brief Array run advanced static non-blocking function for the interface 'default'.
 * 
 *
 * Schedule to run the array of actions on the array of engines, and return immediately.
 * The length of interface_actions must match the size of engarray.
 * The status of the run can be checked either by ::max_wait or ::max_nowait;
 * note that one of these *must* be called, so that associated memory can be released.
 *
 * \param [in] engarray The array of devices to use.
 * \param [in] interface_actions The array of actions to run.
 * \return A handle on the execution status of the actions, or NULL in case of error.
 */
max_run_t *TensorAddition_run_array_nonblock(max_engarray_t *engarray, TensorAddition_actions_t *interface_actions[]);

/**
 * \brief Converts a static-interface action struct into a dynamic-interface max_actions_t struct.
 *
 * Note that this is an internal utility function used by other functions in the static interface.
 *
 * \param [in] maxfile The maxfile to use.
 * \param [in] interface_actions The interface-specific actions to run.
 * \return The dynamic-interface actions to run, or NULL in case of error.
 */
max_actions_t* TensorAddition_convert(max_file_t *maxfile, TensorAddition_actions_t *interface_actions);

/**
 * \brief Initialise a maxfile.
 */
max_file_t* TensorAddition_init(void);

/* Error handling functions */
int TensorAddition_has_errors(void);
const char* TensorAddition_get_errors(void);
void TensorAddition_clear_errors(void);
/* Free statically allocated maxfile data */
void TensorAddition_free(void);
/* returns: -1 = error running command; 0 = no error reported */
int TensorAddition_simulator_start(void);
/* returns: -1 = error running command; 0 = no error reported */
int TensorAddition_simulator_stop(void);

#ifdef __cplusplus
}
#endif /* __cplusplus */
#endif /* SLIC_DECLARATIONS_TensorAddition_H */

#endif /* SLIC_NO_DECLARATIONS */

#ifdef PHOTON_NODE_DATA
#define PHOTON_NODE_DATA_PRESENT 1
PHOTON_NODE_DATA(TensorAdditionKernel, 8, NodeInputMappedReg, "Scalar input (io_s_force_disabled)", "com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.IO.output(IO.java:816)\ntensoraddition.TensorAdditionKernelSimple.<init>(TensorAdditionKernelSimple.maxj:22)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:21)\n")
PHOTON_NODE_DATA(TensorAdditionKernel, 9, NodeNot, "~", "com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.IO.output(IO.java:816)\ntensoraddition.TensorAdditionKernelSimple.<init>(TensorAdditionKernelSimple.maxj:22)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:21)\n")
PHOTON_NODE_DATA(TensorAdditionKernel, 0, NodeInputMappedReg, "Scalar input (io_x_force_disabled)", "com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.IO.input(IO.java:606)\ntensoraddition.TensorAdditionKernelSimple.<init>(TensorAdditionKernelSimple.maxj:15)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:21)\n")
PHOTON_NODE_DATA(TensorAdditionKernel, 1, NodeNot, "~", "com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.IO.input(IO.java:606)\ntensoraddition.TensorAdditionKernelSimple.<init>(TensorAdditionKernelSimple.maxj:15)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:21)\n")
PHOTON_NODE_DATA(TensorAdditionKernel, 2, NodeInput, "Input(x)", "com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.IO.input(IO.java:606)\ntensoraddition.TensorAdditionKernelSimple.<init>(TensorAdditionKernelSimple.maxj:15)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:21)\n")
PHOTON_NODE_DATA(TensorAdditionKernel, 3, NodeInputMappedReg, "Scalar input (io_y_force_disabled)", "com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.IO.input(IO.java:606)\ntensoraddition.TensorAdditionKernelSimple.<init>(TensorAdditionKernelSimple.maxj:16)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:21)\n")
PHOTON_NODE_DATA(TensorAdditionKernel, 4, NodeNot, "~", "com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.IO.input(IO.java:606)\ntensoraddition.TensorAdditionKernelSimple.<init>(TensorAdditionKernelSimple.maxj:16)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:21)\n")
PHOTON_NODE_DATA(TensorAdditionKernel, 5, NodeInput, "Input(y)", "com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.IO.input(IO.java:606)\ntensoraddition.TensorAdditionKernelSimple.<init>(TensorAdditionKernelSimple.maxj:16)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:21)\n")
PHOTON_NODE_DATA(TensorAdditionKernel, 6, NodeAdd, "+", "com.maxeler.maxcompiler.v2.kernelcompiler.types.base.DFEVar.add(DFEVar.java:876)\ntensoraddition.TensorAdditionKernelSimple.<init>(TensorAdditionKernelSimple.maxj:20)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:21)\n")
PHOTON_NODE_DATA(TensorAdditionKernel, 11, NodeOutput, "Output(s)", "com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.IO.output(IO.java:816)\ntensoraddition.TensorAdditionKernelSimple.<init>(TensorAdditionKernelSimple.maxj:22)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:21)\n")
PHOTON_NODE_DATA(TensorAdditionKernel, 16, NodeConstantRawBits, "{HWOffsetFix:1, 0, UNSIGNED}\n0x1; 1.0", "com.maxeler.maxcompiler.v2.managers.DFEManager.build(DFEManager.java:373)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:32)\n")
PHOTON_NODE_DATA(TensorAdditionKernel, 25, NodeConstantRawBits, "{HWOffsetFix:1, 0, UNSIGNED}\n0x1; 1.0", "com.maxeler.maxcompiler.v2.managers.DFEManager.build(DFEManager.java:373)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:32)\n")
PHOTON_NODE_DATA(TensorAdditionKernel, 13, NodeConstantRawBits, "{HWOffsetFix:49, 0, UNSIGNED}\n0x1000000000000; 2.81474976710656E14", "com.maxeler.maxcompiler.v2.managers.DFEManager.build(DFEManager.java:373)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:32)\n")
PHOTON_NODE_DATA(TensorAdditionKernel, 14, NodeCounterV1, "Counter(NUMERIC_INCREMENTING)\nInc: 1\nReset: 0\nInit: 0", "com.maxeler.maxcompiler.v2.managers.DFEManager.build(DFEManager.java:373)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:32)\n")
PHOTON_NODE_DATA(TensorAdditionKernel, 15, NodeStreamOffset, "stream offset: 1", "com.maxeler.maxcompiler.v2.managers.DFEManager.build(DFEManager.java:373)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:32)\n")
PHOTON_NODE_DATA(TensorAdditionKernel, 17, NodeOutputMappedReg, "Scalar output (current_run_cycle_count)", "com.maxeler.maxcompiler.v2.managers.DFEManager.build(DFEManager.java:373)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:32)\n")
PHOTON_NODE_DATA(TensorAdditionKernel, 24, NodeConstantRawBits, "{HWOffsetFix:1, 0, UNSIGNED}\n0x1; 1.0", "com.maxeler.maxcompiler.v2.managers.DFEManager.build(DFEManager.java:373)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:32)\n")
PHOTON_NODE_DATA(TensorAdditionKernel, 19, NodeConstantRawBits, "{HWOffsetFix:49, 0, UNSIGNED}\n0x1000000000000; 2.81474976710656E14", "com.maxeler.maxcompiler.v2.managers.DFEManager.build(DFEManager.java:373)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:32)\n")
PHOTON_NODE_DATA(TensorAdditionKernel, 20, NodeCounterV1, "Counter(NUMERIC_INCREMENTING)\nInc: 1\nReset: 0\nInit: 0", "com.maxeler.maxcompiler.v2.managers.DFEManager.build(DFEManager.java:373)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:32)\n")
PHOTON_NODE_DATA(TensorAdditionKernel, 22, NodeInputMappedReg, "Scalar input (run_cycle_count)", "com.maxeler.maxcompiler.v2.managers.DFEManager.build(DFEManager.java:373)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:32)\n")
PHOTON_NODE_DATA(TensorAdditionKernel, 23, NodeEq, "==", "com.maxeler.maxcompiler.v2.managers.DFEManager.build(DFEManager.java:373)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:32)\n")
PHOTON_NODE_DATA(TensorAdditionKernel, 21, NodeFlush, "flush on trigger", "com.maxeler.maxcompiler.v2.managers.DFEManager.build(DFEManager.java:373)\ntensoraddition.TensorAdditionManager.main(TensorAdditionManager.maxj:32)\n")
#endif

#ifdef SLIC_USE_DEFINITIONS
#include <stdio.h>
#include <math.h>
#include <pthread.h>
#include <string.h>
#include <unistd.h>
#include <stdlib.h>
static max_file_t *stored_maxfile = NULL;
static max_engine_t *stored_engine = NULL;
static char *stored_error = NULL;
static int stored_has_error = 0;
static pthread_once_t slic_bs_is_initialised = PTHREAD_ONCE_INIT;

static void set_error(const char *error_str)
{
	stored_has_error = 1; 
	if(stored_error == NULL) {
		stored_error = strdup(error_str);
	} else {
		char *nerr = malloc(strlen(stored_error) + strlen(error_str) + 2);
		sprintf(nerr, "%s\n%s", stored_error, error_str);
		free(stored_error);
		stored_error = nerr;
	}
}
static void set_error_and_free(char *error_str){
	set_error(error_str);
	free(error_str);
}
int TensorAddition_has_errors(void)
{	return stored_has_error; }
const char* TensorAddition_get_errors(void)
{	return stored_error; }
void TensorAddition_clear_errors(void)
{
	free(stored_error);
	stored_error = NULL;
	stored_has_error = 0;
}

static char TensorAddition_use_simulation[16];
static void TensorAddition_def_use_simulation(void)
{
	long pid = ((long) getpid()) % 100000;
	snprintf(TensorAddition_use_simulation, 16, "TensorAd_%05ld_", pid);
}
static const char *TensorAddition_check_use_simulation(void)
{
	TensorAddition_def_use_simulation();
	const char *use_sim = max_config_get_string(MAX_CONFIG_USE_SIMULATION);
	if (use_sim == NULL) {
		use_sim = TensorAddition_use_simulation;
		max_config_set_string(MAX_CONFIG_USE_SIMULATION, use_sim);
	}
	return use_sim;
}

static int TensorAddition_simulation_launch = 0;
int TensorAddition_simulator_start(void)
{
	int retval = 0;
	const char *use_sim = TensorAddition_check_use_simulation();
	char buff[1024];
	snprintf(buff, 1024, "PATH=simutils:$PATH maxcompilersim -d 1 -n %s -c MAX3424A -S simutils restart", use_sim);
	FILE *pipe_fp = popen(buff, "r");
	if (pipe_fp == NULL) {
		strncat(buff, " : failed to execute.", (1024 - strlen(buff)));
		set_error(buff);
		return -1;
	}
	while (fgets(buff, 1024, pipe_fp) != NULL) {
		/* Uncomment this to get simulator command output */
		/* fprintf(stderr, buff); */
		if (strstr(buff, "Error")) {
			set_error(buff);
			retval = -1;
		}
	}
	pclose(pipe_fp);
	return retval;
}

int TensorAddition_simulator_stop(void)
{
	const char *use_sim = TensorAddition_check_use_simulation();
	char buff[1024];
	snprintf(buff, 1024, "PATH=simutils:$PATH maxcompilersim -d 1 -n %s -c MAX3424A -S simutils stop", use_sim);
	FILE *pipe_fp = popen(buff, "r");
	if (pipe_fp == NULL) {
		strncat(buff, " : failed to execute.", (1024 - strlen(buff)));
		set_error(buff);
		return -1;
	}
	while (fgets(buff, 1024, pipe_fp) != NULL) {
		/* Uncomment this to get simulator command output */
		/* fprintf(stderr, buff); */
		;
	}
	pclose(pipe_fp);
	return 0;
}

static void TensorAddition_static_init(void) 
{
	stored_maxfile = TensorAddition_init();
	if (stored_maxfile == NULL || !max_ok(stored_maxfile->errors)) {
		stored_maxfile = NULL;
		if(max_config_get_bool(MAX_CONFIG_STATIC_INTERFACE_ABORT_ON_ERROR)) abort();
		else { set_error("Unable to load maxfile"); return; }
	}
	if(!max_ok(max_global_errors())) {
		set_error_and_free(max_errors_trace(max_global_errors()));
		return;
	}
	if(!max_config_get_bool(MAX_CONFIG_STATIC_INTERFACE_ABORT_ON_ERROR))
		max_errors_mode(stored_maxfile->errors, 0);
	time_t timeout_previous = max_load_timeout(stored_maxfile, 30);
	const char *use_sim = TensorAddition_check_use_simulation();
	if (max_ping_daemon(stored_maxfile, use_sim) == 0) {
		int sim_stat = TensorAddition_simulator_start();
		if ((sim_stat == 0) && (max_ping_daemon(stored_maxfile, use_sim) == 1)) {
			TensorAddition_simulation_launch = 1;
		} else {
			set_error("Error: An error occurred while trying to start the simulation infrastructure automatically.");
			set_error("Error: Check that 'use_simulation=<simulator_name>' is set correctly in your SLiC configuration");
			set_error("Error: and that the associated simulated system daemon is running.");
			max_file_free(stored_maxfile);
			stored_maxfile = NULL;
			return;
		}
	}
	stored_engine = max_load(stored_maxfile, "*");
	if (!max_ok(stored_maxfile->errors)) {
		if(max_config_get_bool(MAX_CONFIG_STATIC_INTERFACE_ABORT_ON_ERROR)) {
			fprintf(stderr, "\nUnable to load engine: aborting now.\n\n");
			fflush(stderr);
			abort();
		} else {
			set_error_and_free(max_errors_trace(stored_maxfile->errors));
			max_file_free(stored_maxfile);
			stored_maxfile = NULL;
			return;
		} 
	} 
	max_load_timeout(stored_maxfile, timeout_previous);
}
void TensorAddition_free(void)
{
	if (stored_engine != NULL) {
		max_unload(stored_engine);
		stored_engine = NULL;
	}
	if (stored_maxfile != NULL) {
		max_file_free(stored_maxfile);
		stored_maxfile = NULL;
	}
	if (stored_error != NULL) {
		free(stored_error);
		stored_error = NULL;
	}
	if (TensorAddition_simulation_launch == 1) {
		int sim_stat = TensorAddition_simulator_stop();
		if (sim_stat != 0 ) {
			fprintf(stderr, "Error stopping simulator.");
		}
		TensorAddition_simulation_launch = 0;
	}
}

static int TensorAddition_get_pcie_alignment(void)
{
#ifdef TensorAddition_PCIE_ALIGNMENT
	return ((TensorAddition_PCIE_ALIGNMENT < 1) ? 16 : TensorAddition_PCIE_ALIGNMENT);
#else
	return 16;
#endif
}

static int TensorAddition_check_aligned(const void *data)
{
	uintptr_t pointer = (uintptr_t) data;
	int alignment = TensorAddition_get_pcie_alignment();
	return (pointer % alignment) ? 1 : 0;
}

static void *TensorAddition_malloc_aligned(const size_t size)
{
	void *ptr;
	int alignment = TensorAddition_get_pcie_alignment();
	posix_memalign(&ptr, alignment, size);
	return ptr;
}



/*----------------------------------------------------------------------------*/
/*---------------------------- Interface default -----------------------------*/
/*----------------------------------------------------------------------------*/



#define CHECK_ERRORS_ST(ST, RET) if(!max_ok(ST->errors)) { if(max_config_get_bool(MAX_CONFIG_STATIC_INTERFACE_ABORT_ON_ERROR)) { fprintf(stderr, "%s\n", max_errors_trace(ST->errors)); abort(); } set_error_and_free(max_errors_trace(ST->errors)); return RET; } 
#define CHECK_NULL(VALUE, MESSAGE, RET) if(VALUE == NULL) { if (max_config_get_bool(MAX_CONFIG_STATIC_INTERFACE_ABORT_ON_ERROR)) { fprintf(stderr, "%s\n%s\n", (stored_error == NULL) ? "" : stored_error, MESSAGE); abort(); } set_error(MESSAGE); return RET; }

 
typedef struct TensorAddition_callback_stream {
	uint8_t *user_ptr;
	uint8_t *aligned_ptr;
	size_t   size;
	int      is_output;
} TensorAddition_callback_stream_t;

typedef struct TensorAddition_callback_data {
	TensorAddition_callback_stream_t stream[3]; 
	int count;
	int max_count;
} TensorAddition_callback_data_t;

static void TensorAddition_callback_internal(void *cb_data)
{
	TensorAddition_callback_data_t *data = (TensorAddition_callback_data_t*) cb_data;
	for (int i = 0 ; i < data->count ; i++ ) {
		TensorAddition_callback_stream_t *s = &data->stream[i];
		if (s->is_output && (s->size > 0)) {
			memcpy(s->user_ptr, s->aligned_ptr, s->size);
		}
		free(s->aligned_ptr);
	}
	free(data);
}

static max_actions_t* TensorAddition_convert_internal(
	max_file_t *maxfile,
	TensorAddition_actions_t *interface_actions,
	int  is_internal_call,
	void (**callback_func)(void*),
	void **callback_data)
{
	max_actions_t *actions = max_actions_init(maxfile, NULL);
	if(actions == NULL) return NULL;

#define CHECK_ERRORS if(!max_ok(actions->errors)) { set_error_and_free(max_errors_trace(actions->errors)); return NULL; } 

	TensorAddition_callback_data_t *cb_data = NULL;
	int use_callback = (callback_func != NULL) && (callback_data != NULL);
	if (use_callback) {
		cb_data = malloc(sizeof(TensorAddition_callback_data_t));
		if (cb_data == NULL) {
			fprintf(stderr, "Unable to allocate memory for stream callback data in function TensorAddition_convert_internal\n");
			return NULL;
		}
		cb_data->max_count = 3;
		cb_data->count     = 0;
		*callback_data     = cb_data;
		*callback_func     = &TensorAddition_callback_internal;
	}

	int64_t param_N = interface_actions->param_N;
	
	/* code for scalar TensorAdditionKernel.run_cycle_count */
	uint64_t ticks_TensorAdditionKernel = param_N;
	max_set_ticks(actions, "TensorAdditionKernel", param_N);
	CHECK_ERRORS;
	/* end of code for scalar TensorAdditionKernel.run_cycle_count*/
	
	int64_t tmp_0 = (param_N * 8);
	
	/* code for stream x */
	size_t instream_size_x = tmp_0;
	if (instream_size_x > 0) {
		const double *stream_ptr = interface_actions->instream_x;
		if (use_callback && (1 == TensorAddition_check_aligned(interface_actions->instream_x))) {
			double *aligned_instream_x = malloc(instream_size_x);
			if (aligned_instream_x == NULL) {
				max_report_error_slic(actions->errors, __FILE__, __LINE__, 526, "Failed to allocate aligned memory for stream 'x'");
				CHECK_ERRORS;
			}
			(&cb_data->stream[cb_data->count])->user_ptr    = (uint8_t*) interface_actions->instream_x;
			(&cb_data->stream[cb_data->count])->aligned_ptr = (uint8_t*) aligned_instream_x;
			(&cb_data->stream[cb_data->count])->size        = instream_size_x;
			(&cb_data->stream[cb_data->count])->is_output   = 0;
			cb_data->count += 1;
			memcpy(aligned_instream_x, interface_actions->instream_x, instream_size_x);
			stream_ptr = aligned_instream_x;
		}
		max_queue_input(actions, "x", stream_ptr, instream_size_x);
		CHECK_ERRORS;
	}
	/* end of code for stream x */
	
	int64_t tmp_1 = (param_N * 8);
	
	/* code for stream y */
	size_t instream_size_y = tmp_1;
	if (instream_size_y > 0) {
		const double *stream_ptr = interface_actions->instream_y;
		if (use_callback && (1 == TensorAddition_check_aligned(interface_actions->instream_y))) {
			double *aligned_instream_y = malloc(instream_size_y);
			if (aligned_instream_y == NULL) {
				max_report_error_slic(actions->errors, __FILE__, __LINE__, 526, "Failed to allocate aligned memory for stream 'y'");
				CHECK_ERRORS;
			}
			(&cb_data->stream[cb_data->count])->user_ptr    = (uint8_t*) interface_actions->instream_y;
			(&cb_data->stream[cb_data->count])->aligned_ptr = (uint8_t*) aligned_instream_y;
			(&cb_data->stream[cb_data->count])->size        = instream_size_y;
			(&cb_data->stream[cb_data->count])->is_output   = 0;
			cb_data->count += 1;
			memcpy(aligned_instream_y, interface_actions->instream_y, instream_size_y);
			stream_ptr = aligned_instream_y;
		}
		max_queue_input(actions, "y", stream_ptr, instream_size_y);
		CHECK_ERRORS;
	}
	/* end of code for stream y */
	
	int64_t tmp_2 = (param_N * 8);
	
	/* code for stream s */
	size_t outstream_size_s = tmp_2;
	if (outstream_size_s > 0) {
		double *stream_ptr = interface_actions->outstream_s;
		if (use_callback && (1 == TensorAddition_check_aligned(interface_actions->outstream_s))) {
			double *aligned_outstream_s = malloc(outstream_size_s);
			if (aligned_outstream_s == NULL) {
				max_report_error_slic(actions->errors, __FILE__, __LINE__, 526, "Failed to allocate aligned memory for stream 's'");
				CHECK_ERRORS;
			}
			(&cb_data->stream[cb_data->count])->user_ptr    = (uint8_t*) interface_actions->outstream_s;
			(&cb_data->stream[cb_data->count])->aligned_ptr = (uint8_t*) aligned_outstream_s;
			(&cb_data->stream[cb_data->count])->size        = outstream_size_s;
			(&cb_data->stream[cb_data->count])->is_output   = 1;
			cb_data->count += 1;
			stream_ptr = aligned_outstream_s;
		}
		max_queue_output(actions, "s", stream_ptr, outstream_size_s);
		CHECK_ERRORS;
	}
	/* end of code for stream s */
	
	if (use_callback && cb_data->count == 0) {
		*callback_data = NULL;
		*callback_func = NULL;
		free(cb_data);
	}
	return actions;
#undef CHECK_ERRORS
}

void TensorAddition(
	int64_t param_N,
	const double *instream_x,
	const double *instream_y,
	double *outstream_s)
{
	(void) pthread_once(&slic_bs_is_initialised, TensorAddition_static_init);
	CHECK_NULL(stored_maxfile, "Maxfile was not loaded", );
	max_run_t *run = TensorAddition_nonblock(param_N, instream_x, instream_y, outstream_s);
	CHECK_NULL(run, "Unable to run actions", );
	max_wait(run);
}

max_run_t *TensorAddition_nonblock(
	int64_t param_N,
	const double *instream_x,
	const double *instream_y,
	double *outstream_s)
{
	TensorAddition_actions_t interface_actions;
	interface_actions.param_N = param_N;
	interface_actions.instream_x = instream_x;
	interface_actions.instream_y = instream_y;
	interface_actions.outstream_s = outstream_s;
	(void) pthread_once(&slic_bs_is_initialised, TensorAddition_static_init);
	CHECK_NULL(stored_maxfile, "Maxfile was not loaded", NULL);
	void (*cb_func)(void*) = NULL;
	void  *cb_data         = NULL;
	max_actions_t *actions = TensorAddition_convert_internal(stored_maxfile, &interface_actions, 1, &cb_func, &cb_data);
	CHECK_NULL(actions, "Unable to build actions", NULL);
	max_validate(actions);
	CHECK_ERRORS_ST(actions, NULL);
	CHECK_ERRORS_ST(stored_engine, NULL);
	max_run_t *run;
	if (cb_func == NULL) {
		run = max_run_nonblock(stored_engine, actions);
	} else {
		run = max_run_nonblock_with_cb(stored_engine, actions, cb_func, cb_data);
	}
	CHECK_NULL(run, "Unable to run actions", NULL);
	CHECK_ERRORS_ST(actions, NULL);
	max_actions_free(actions);
	return run;
}

void TensorAddition_run(
	max_engine_t *engine,
	TensorAddition_actions_t *interface_actions)
{
	max_run_t *run = TensorAddition_run_nonblock(engine, interface_actions);
	CHECK_NULL(run, "Unable to run actions", );
	max_wait(run);
}

max_run_t *TensorAddition_run_nonblock(
	max_engine_t *engine,
	TensorAddition_actions_t *interface_actions)
{
	max_file_t *maxfile = max_engine_get_max_file(engine); 
	void (*cb_func)(void*) = NULL;
	void  *cb_data         = NULL;
	max_actions_t *actions = TensorAddition_convert_internal(maxfile, interface_actions, 1, &cb_func, &cb_data);
	CHECK_NULL(actions, "Unable to build actions", NULL);
	max_validate(actions);
	CHECK_ERRORS_ST(actions, NULL);
	max_run_t *run;
	if (cb_func == NULL) {
		run = max_run_nonblock(engine, actions);
	} else {
		run = max_run_nonblock_with_cb(engine, actions, cb_func, cb_data);
	}
	CHECK_NULL(run, "Unable to run actions", NULL);
	max_actions_free(actions);
	return run;
}


/**
 * \brief Group run advanced static function for the interface 'default'.
 * 
 * \param [in] group Group to use.
 * \param [in,out] interface_actions Actions to run.
 *
 * Run the actions on the first device available in the group.
 */
void TensorAddition_run_group(max_group_t *group, TensorAddition_actions_t *interface_actions)
{
	max_run_t *run = TensorAddition_run_group_nonblock(group, interface_actions);
	CHECK_NULL(run, "Unable to run actions", );
	max_wait(run);
}


/**
 * \brief Group run advanced static non-blocking function for the interface 'default'.
 * 
 *
 * Schedule the actions to run on the first device available in the group and return immediately.
 * The status of the run must be checked with ::max_wait. 
 * Note that use of ::max_nowait is prohibited with non-blocking running on groups:
 * see the ::max_run_group_nonblock documentation for more explanation.
 *
 * \param [in] group Group to use.
 * \param [in] interface_actions Actions to run.
 * \return A handle on the execution status of the actions, or NULL in case of error.
 */
max_run_t *TensorAddition_run_group_nonblock(max_group_t *group, TensorAddition_actions_t *interface_actions)
{
	max_file_t *maxfile = max_group_get_max_file(group);
	max_actions_t *actions = TensorAddition_convert_internal(maxfile, interface_actions, 1, NULL, NULL);
	if(actions == NULL) return NULL;
	if(!max_ok(actions->errors)) return NULL;
	max_validate(actions);
	max_run_t *run = max_run_group_nonblock(group, actions);
	max_actions_free(actions);
	return run;
}


/**
 * \brief Array run advanced static function for the interface 'default'.
 * 
 * \param [in] engarray The array of devices to use.
 * \param [in,out] interface_actions The array of actions to run.
 *
 * Run the array of actions on the array of engines.  The length of interface_actions
 * must match the size of engarray.
 */
void TensorAddition_run_array(max_engarray_t *engarray, TensorAddition_actions_t *interface_actions[])
{
	max_run_t *run = TensorAddition_run_array_nonblock(engarray, interface_actions);
	CHECK_NULL(run, "Unable to run actions", );
	max_wait(run);
}


/**
 * \brief Array run advanced static non-blocking function for the interface 'default'.
 * 
 *
 * Schedule to run the array of actions on the array of engines, and return immediately.
 * The length of interface_actions must match the size of engarray.
 * The status of the run can be checked either by ::max_wait or ::max_nowait;
 * note that one of these *must* be called, so that associated memory can be released.
 *
 * \param [in] engarray The array of devices to use.
 * \param [in] interface_actions The array of actions to run.
 * \return A handle on the execution status of the actions, or NULL in case of error.
 */
max_run_t *TensorAddition_run_array_nonblock(max_engarray_t *engarray, TensorAddition_actions_t *interface_actions[])
{
	max_file_t *maxfile = max_engarray_get_max_file(engarray, 0);
	int i;
	max_actarray_t *actarray = max_actarray_init(maxfile, engarray->size);
	if (actarray == NULL) return NULL;
	max_actions_t **arr_actions = malloc(engarray->size * sizeof(max_actions_t*));
	for ( i = 0 ; i < actarray->size; i++ ) {
		max_actions_t *actions = TensorAddition_convert_internal(maxfile, interface_actions[i], 1, NULL, NULL);
		if (actions == NULL) return NULL;
		arr_actions[i] = actions;
		max_set_action(actarray, i, actions);
	}
	max_run_t *run = max_run_array_nonblock(engarray, actarray);
	for ( i = 0 ; i < actarray->size ; i++ ) { max_actions_free(arr_actions[i]); }
	max_actarray_free(actarray);
	free(arr_actions);
	return run;
}


/**
 * \brief Converts a static-interface action struct into a dynamic-interface max_actions_t struct.
 *
 * Note that this is an internal utility function used by other functions in the static interface.
 *
 * \param [in] maxfile The maxfile to use.
 * \param [in] interface_actions The interface-specific actions to run.
 * \return The dynamic-interface actions to run, or NULL in case of error.
 */
max_actions_t* TensorAddition_convert(max_file_t *maxfile, TensorAddition_actions_t *interface_actions)
{
	return TensorAddition_convert_internal(maxfile, interface_actions, 0, NULL, NULL);
}

#undef CHECK_ERRORS_ST
#undef CHECK_NULL



#endif /* SLIC_USE_DEFINITIONS */

#ifdef SLIC_DYNAMIC_CODE
SLIC_MODE_START(default)	
SLIC_PARAMETER_UINT64(int64_t, N, param_N)	
SLIC_SET_TICKS(TensorAdditionKernel, param_N)	
SLIC_TEMP_VAR(int64_t tmp_0 = (param_N * 8);)	
SLIC_STREAM(x, tmp_0)	
SLIC_TEMP_VAR(int64_t tmp_1 = (param_N * 8);)	
SLIC_STREAM(y, tmp_1)	
SLIC_TEMP_VAR(int64_t tmp_2 = (param_N * 8);)	
SLIC_STREAM(s, tmp_2)	
SLIC_MODE_END(default)

#endif /* SLIC_DYNAMIC_CODE */

#ifdef SKIN_META_DATA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#endif /* SKIN_META_DATA */

#ifdef SLIC_B64_DEFINITIONS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#endif /* SLIC_B64_DEFINITIONS */

#ifdef SLIC_EXTRA_FILES
PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiIHN0YW5kYWxvbmU9Im5vIj8+PHVz
ZXJmaWxlcyBmb3JtYXQtdmVyc2lvbj0iMjAxMjAyMDAiLz4=
#endif /* SLIC_EXTRA_FILES */

#ifdef PHOTON_NODE_ADD_DATA
#define PHOTON_NODE_ADD_DATA_PRESENT 1
PHOTON_NODE_ADD_DATA(TensorAdditionKernel, 8, "SquashFactor", 0.0)
PHOTON_NODE_ADD_DATA(TensorAdditionKernel, 9, "SquashFactor", 1.0)
PHOTON_NODE_ADD_DATA(TensorAdditionKernel, 0, "SquashFactor", 0.0)
PHOTON_NODE_ADD_DATA(TensorAdditionKernel, 1, "SquashFactor", 1.0)
PHOTON_NODE_ADD_DATA(TensorAdditionKernel, 2, "SquashFactor", 0.0)
PHOTON_NODE_ADD_DATA(TensorAdditionKernel, 3, "SquashFactor", 0.0)
PHOTON_NODE_ADD_DATA(TensorAdditionKernel, 4, "SquashFactor", 1.0)
PHOTON_NODE_ADD_DATA(TensorAdditionKernel, 5, "SquashFactor", 0.0)
PHOTON_NODE_ADD_DATA(TensorAdditionKernel, 6, "SquashFactor", 0.0)
PHOTON_NODE_ADD_DATA(TensorAdditionKernel, 11, "SquashFactor", 0.0)
PHOTON_NODE_ADD_DATA(TensorAdditionKernel, 16, "SquashFactor", 0.0)
PHOTON_NODE_ADD_DATA(TensorAdditionKernel, 25, "SquashFactor", 0.0)
PHOTON_NODE_ADD_DATA(TensorAdditionKernel, 13, "SquashFactor", 0.0)
PHOTON_NODE_ADD_DATA(TensorAdditionKernel, 14, "SquashFactor", 0.0)
PHOTON_NODE_ADD_DATA(TensorAdditionKernel, 15, "SquashFactor", 0.0)
PHOTON_NODE_ADD_DATA(TensorAdditionKernel, 17, "SquashFactor", 0.0)
PHOTON_NODE_ADD_DATA(TensorAdditionKernel, 24, "SquashFactor", 0.0)
PHOTON_NODE_ADD_DATA(TensorAdditionKernel, 19, "SquashFactor", 0.0)
PHOTON_NODE_ADD_DATA(TensorAdditionKernel, 20, "SquashFactor", 0.0)
PHOTON_NODE_ADD_DATA(TensorAdditionKernel, 22, "SquashFactor", 0.0)
PHOTON_NODE_ADD_DATA(TensorAdditionKernel, 23, "SquashFactor", 0.0)
PHOTON_NODE_ADD_DATA(TensorAdditionKernel, 21, "SquashFactor", 0.0)
#endif

#ifdef MAXFILE_SIGNATURE
#define MAXFILE_SIGNATURE_PRESENT 1
MAXFILE_SIGNATURE("302d021500910bdd74565a0a98df4473306151c390ba08001a02140690ccf7c91783b6d9581d69eb8f096464010bea")
#endif

